Ultimate Gestalt List

+ AI / ML
Machine Learning
Supervised, Unsupervised, Semi-Supervised, and Self-Supervised Learning
Meta-Learning
Reinforcement Learning
Self-Organizing Maps
Mathematics for Machine Learning
Linear Algebra
Linear and Nonlinear Programming 
Applied Matrix Theory
Artificial Neural Networks
Convolutional Neural Networks
Neuroevolution
Evolutionary Programming
Evolutionary Strategies
Quality Diversity
Deep Learning
Recurrent Neural Network
Transformers
Dimensionality Reduction
Kernel and Kernel Machines
NLPs and LLMs
LSTMs
GPTs
GANs
Self-Atttention
Design Patterns
MARLs
TensorFlow, PyTorch, Jax, 
Network Theory
Python programming
SVMs

+ Complexity and Systems Theory 
Waldrop Complexity 
Minsky "The Society of Mind"
E. O. Wilson "The Ants"
Krugman "The self-organizing economy"
God and Golem Inc - Weiner
Kevin Kelly "Out of Control"
Hillis "The Pattern on the Stone"
"Fire in the Mind" George Johnson
"Adventures of a Mathematician" Stanislaw Ulam
Robert Shaw, Chaos
Hao BaiLin - "Chaos" and Cvitanovic "Universality in Chaos"
Turings "The Chemical Basis of Morphogenesis"
Harvey Gold "Mathematical Modeling of Biological Systems"
Mandlebrot "The Fractal Geometry of Nature"
"Fractal Time: Why a watched pot never boils"
D'Arcy Thompson: "On Growth and Form"
Thompson "Nonlinear Dynamics and Chaos"
Kauffman S. A. The Origins of Order: Self-Organization and Selection in Evolution
Holland J. H. 1992. Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence
Maturana H. R., & Varela F. J. The Tree of Knowledge: The Biological Roots of Understanding
Prigogine, I. and Stengers, I. Order out of Chao
Anderson P. W., K. J. Arrow, and D. Pines (Eds.). The Economy as an Evolving Complex System
Ashby W. R. An Introduction to Cybernetic
Dawkins R. The selfish gene
Eigen M. and P. Schuster. The Hypercycle: A principle of natural self- organization
Kauffman S. A.: Antichaos and Adaptation
Langton, C. G. (Ed.). Artificial Life: The Proceedings of an Interdisciplinary Workshop on the Synthesis and Simulation of Living Systems
Nicolis, G, and Prigogine, I. Self-Organization in Non-Equilibrium Systems
Simon, H. A. The Sciences of the Artificial
von Foerster H. Cybernetics of Cybernetics (2nd edition)
Waldrop M. M. Complexity: The Emerging Science at the Edge of Order and Chaos
Wiener N. Cybernetics: Or Control and Communication in the Animal and the Machin
Zeleny M. (Ed.) 1981, Autopoiesis: A Theory of Living Organization
Neumann, Goldstine, Burks - "Preliminary Discussion of Logic and Design of Electronic Computing Instrument"
Holland - "A Logical Theory of Adaptive Systems Informally Described"
Holland - "Adaptation in Natural and Artificial Systems"
Holland's ECHO among others 
Neumann - "Theory of Self-Reproducing Automata"
Wolfram - "A New Kind of Science"
Allen Newell and Herb Simon - Problem space and heuristics
David Goldberg's Gas pipeline classifier system
Todd Codd's 8-state periodic emitter
Farmer - "Rosetta Stone of Connecitonism"
Kauffmann - "The Origins of Order"
Packard - "Adaptation to the Edge of Chaos"
Per Bak's Self-Organized Criticality
Walter Fontana's Algorithmic Chemistry

Grow Intelligence

Alan Turing
Ilya Prigogine
Stephen Smale
Phillip Marcus
AN Kolmogorov and Yasha Sinai
Goethe, Rudolf Steiner, Schwenk
Poincare
Lorenz, Yorke, Swinney
Claude Shannon
Douglas Hofstadter
Main People:
Brian Arthur (Economics, Nonlinear Systems)
John Holland (Everything)
Stuart Kauffman (Networks, Genetic Circuits)
Chris Langton (A. Life)
Packard and Farmer (Chaos, Dynamical Systems Collaborative)
Stephen Wolfram (Automata)
Arthur Burks, Goldstine, Von Neumann (Automata, early computers)
Other People: 
George Cowan, David Pines, Stirling Colgate, Murray Gell-Mann, Nick Metropolis, Herb Anderson, Peter A. Carruthers, Ken Arrow
Erwin Laszlo, Von Bertalanffy, Anatol Rapoport, Howard H. Pattee, Robert Rosen

Feigenbaum Universality, Libchaber
Barnsley, Ruelle, Julia Sets
Nonlinear Studies
Chaos
Lyapunov Exponents
Phase Space Reconstruction, strange attractors, fractal dimensions, Poincare maps
Santa Fe Institute
Complexity Theory, Complex Systems, Complex Adaptive Systems, Nonlinear Dynamics

Nonlinear systems
Dynamical systems - Chaos Theory, Self-Assembly, Self-Organization, Edge-of-Chaos
Tensors
Neural Networks
Complexity Science
Brian Arthur Nonlinear dynamic processes
Evolutinary Programming
Markov Processes
Computational Biology
Cellular Automata
Nonlinear Mathematics / Nonlinear Dynamics
Networks
Artificial Life
Theory of Complex Systems
Self-Organization

Books and Journals 

"Networks: An Introduction" by Newman
"A Mathematical Theory of Communication" by Shannon

"Towards a general theory of adaptive walks on rugged landscapes" by Kauffman
"Metabolic stability and epigenesis in randomly constructed genetic nets" by Kauffman 
"The NK model of rugged fitness landscapes and  its application to maturation of the immune response" by Kauffman
"The Origins of Order: Self-Organization and Selection in Evolution" by Kauffman S. A. 
"Antichaos and Adaptation by" Kauffman S. A.
"The Origins of Order" by Kauffmann

"A Logical Theory of Adaptive Systems Informally Described" by Holland
"Adaptation in Natural and Artificial Systems" by Holland
"Hidden Order" by Holland

"Chaos" by Robert Shaw
"Order Out of Chaos" by Prigogine, I. and Stengers, I. 
"Chaos" by Hao BaiLin 
"Chaos: A view of complexity in the physical sciences" by L. Kadanoff
"Universal Behavior in nonlinear systems" by M. Feigenbaum
"On the importance of nonlinear modeling in computer performance prediction" by Garland and Bradley
"Universality in Chaos" by Cvitanovic
"Nonlinear Dynamics and Chaos" by Thompson 
"Nonlinear Dynamics and Chaos" by Strogatz
Die Erforschung des Chaos: Eine Einf√ºhrung in die Theorie nichtlinearer Systeme
John Argyris, Gunter Faust, Maria Haase und Rudolf Friedrich
"Adaptation to the Edge of Chaos" by Packard
"Computation at the Edge of Chaos: phase transitions and emergent computation" by Langton C. G.

"Robust space-time intermittency and 1/f noise" by Keeler, J., and Farmer, J. D.
"Rosetta Stone of Connecitonism" by Farmer

"The Fractal Geometry of Nature" by Mandelbrot
"Fractal Time: Why a watched pot never boils" by Susie Vrobel
"Introducing Fractal Geometry" by Lesmoir-Gordon and R. Edney
"Fractals, Chaos, Power Laws" by W.H. Freeman

"Self-organized criticality: an explanation of 1/f noise" by Bak, P., Tang, C., and Wiesenfeld, K. 
"Self-Organized Criticality" by Per Bak
"Boltzmann's entropy and time's arrow" by J.L Lebowitz
"From Eternity to here: The Quest for the Ultimate Theory of Time" by S Carroll
"Information Theory Primer" by T.D. Schneider
"Elements of Information Theory" by Thomas Cover, Joy A. Thomas
"Genetic Programming I" by John R. Koza (I, II, II)

"The Economy as an Evolving Complex System" by Anderson P. W., K. J. Arrow, and D. Pines (Eds.).
"The self-organizing economy" by Krugman
"Competing Technologies, Increasing Returns, and Lock-in by Historical Events" by Arthur W. B. The Economic Journal 99: 1989, pp. 106-131. *
"Positive Feedbacks in the Economy, Scientific American" by Arthur W. B. February 1990, pp. 92-99. *
"Increasing Returns and Path Dependence in the Economy" by Arthur W. B. University of Michigan Press, Ann Arbor, 1994.
"Bounded Rationality and Inductive Behavior (the El Farol Problem)" by Arthur W. B. American Economic Review 84, pp. 406-411, 1994.
"Ecodynamics: a new theory of societal evolution" by Boulding K. E.
"World Dynamics (2nd ed.)" by Forrester, J. W., Wright-Allen Press, Cambridge, MA, 1973. 

"Cybernetics of Cybernetics (2nd edition)" by von Foerster H. 
"An Introduction to Cybernetics" by Ashby W. R. 
"Design for a Brain - The Origin of Adaptive Behaviour" by Ashby, W. R. 
"Cybernetics" by N. Wiener
"Cybernetics: Or Control and Communication in the Animal and the Machine" by Wiener N. 
"The Cybernetic Laws of Social Progress" by Aulin
"General System Theory (Revised Edition)" by von Bertalanffy L.
"On self-organising systems and their environments" by von Foerster H. 
"Principles of Self-Organization" by von Foerster H. 
"Observing Systems: Selected papers" by von Foerster H. 

"Dissipation, Information, Computational Complexity and the Definition of Organization. Emerging Syntheses in Science" by Bennett C. H. 
"Autopoiesis: A Theory of Living Organization" by Zeleny M. (Ed.) 1981, 
"Self Organization in Biological Systems" by Eric Bonabeau 
"Self-Organization in Non-Equilibrium Systems" by Nicolis, G, and Prigogine, I.
"The Hypercycle: A principle of natural self- organization" by Eigen M. and P. Schuster. 
"Mathematical Modeling of Biological Systems" by Harvey Gold 
"Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence" by Holland J. H. 1992. 
"The Tree of Knowledge: The Biological Roots of Understanding" by Maturana H. R., & Varela F. J.
"Eigen M., and R. Winkler-Oswatitsch. Steps Towards Life: A Perspective on Evolution." by Eigen M. Oxford University Press, New York, 1992. *
"Fisher R. A. The Genetical Theory of Natural Selection" by Fisher R.A 2nd edition, Dover Publications, New York, 1958. 
"Complexity, Entropy, and the Physics of Information" by Zurek
"Probing the power of an electronic Maxwell's Demon: Single-Electron transistor monitored by a quantum point contact" by Shaller, Emary, Kiesslich, Brandes
"Life's demons: information and order in biology" by Philippe M Binder and Danchin
"Maxwell's Demon 1" and "Maxwell's Demon 2" by Harvery Leff and Andrew Rex
"Mathematical Biology: An Introduction" by James D. Murray

"Surfaces and Essence" by Hofstadter 
"The Minds I" by Hofstadter
"Metamagical Themas" by Hofstadter
"Godel, Escher, Bach" by Hofstadter

"Complexity: The Emerging Science at the Edge of Order and Chaos" by Waldrop M. M. 
"Science and Complexity" by Warren Weaver
"Measures of Complexity" by S. Lloyd
"On Growth and Form" by DArcy Thompson
"Artificial Life: The Proceedings of an Interdisciplinary Workshop on the Synthesis and Simulation of Living Systems" by Langton, C. G. (Ed.). 
"The Sciences of the Artificial" by Simon, H. A. 
"Preliminary Discussion of Logic and Design of Electronic Computing Instrument" by Neumann, Goldstine, Burks - 
"Theory of Self-Reproducing Automata" by Neumann
"A New Kind of Science" by Wolfram
"Problem space and heuristics" by Allen Newell and Herb Simon
"The Selfish Gene" by Dawkins R.
"God and Golem Inc" by Weiner
"Out of Control" by Kevin Kelly 
"The Pattern on the Stone" by Hillis 
"Fire in the Mind" by George Johnson
"Adventures of a Mathematician" by Stanislaw Ulam 
"The Chemical Basis of Morphogenesis" by Turing
"The Society of Mind" by Minsky
"The Ants" by E. O. Wilson 
"The Complexity of Cooperation" by Axelrod
"Mechanisms of Intelligence: Writings of Ross Ashby, Intersystems," Salinas CA, 1981 by Ashby W. R.
"Algorithmic Chemistry" by Walter Fontana
"Emergent Computation" MIT / North-Holland
"Fire in the Mind" by Johnson
"Artificial Life" by Levy
"The Recursive Universe" by Poundstone
"The Quark and the Jaguar: Adventures in the Simple and the Complex" by Gell-Mann
"Evolutionary epistemology, rationality, and the sociology of knowledge" by Campbell, D. T
"Downward Causation in Hierarchically Organized Biological Systems" by Campbell, D. T.
"Complexification: explaining a paradoxical world through the science of surprise" by Casti J.L
"Cellular Automata and Complexity: Collected Papers" by Wolfram
"The Evolution of Complexity" by Heylighen
Systems View of the World - Ervon Laszlo

"Networks: An Introduction" by Newman
"A Mathematical Theory of Communication" by Shannon
"The Origins of Order: Self-Organization and Selection in Evolution" by Kauffman S. A. 
"A Logical Theory of Adaptive Systems Informally Described" by Holland
"Adaptation in Natural and Artificial Systems" by Holland
"Hidden Order" by Holland
"Nonlinear Dynamics and Chaos" by Strogatz
"An Introduction to Cybernetics" by Ashby W. R. 
"Dissipation, Information, Computational Complexity and the Definition of Organization. Emerging Syntheses in Science" by Bennett C. H. 
"Self Organization in Biological Systems" by Eric Bonabeau 
"Self-Organization in Non-Equilibrium Systems" by Nicolis, G, and Prigogine, I.
"The Evolution of Complexity" by Heylighen

Journals:
Wolfram's Complex Systems
Journal of Complexity 
Journal of Systems Science and Complexity
Advances in Complex Systems
Mathematics and Mechanics of Complex Systems
Complex Adaptive Systems Modeling
Journal of Complex Networks
Network and Complex Systems
Complexity Theory
Complexity

*** Complete Treatment ***
Networks: An Introduction by Newman - July
Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering by Steven H. Strogatz - August
Self-Organization in Complex Ecosystems by Ricard V. Sol√© and Jordi Bascompte - September
An Introduction to the Theory of Complex Systems by Hanel, Thurner, Klimek - October

*** Optional ***
Introduction to the Modeling and Analysis of Complex Systems by Hiroki Sayama
Emergent Computation by Forrest
An Introduction to Cybernetics by Ashby
Artificial Life by Levy

*** If/When ***
Hidden Order by Holland
At Home in the Universe by Kauffman
Thinking in Systems by Meadows
The Systems View of the World: A Holistic Vision for Our Time by Ervin Laszlo
Historical Dynamices by Turchin
CAS by Miller
Machine Learning by MIT Press
The Swerve by Greenblatt
A Brief History of Time by Hawking
Philosophy of Complex Systems by Hooker?
The Cybernetic Brain by Pickering

Neuroevolution

- Goals
Start blogging on these topics on medium. turn out a little bit of content
Publication.
Cultivate a resume for a grad school program
Grad school program
Job as a research scientist somewhere cool

- Problem Types
Fractured Problems, (Kohl and Mikkulainen 2009)

- Uncategorized 
Cliff et all 1993
Nolfi and Floreano 2000
Doncieux et al 2015

- Genetic Algorithms
De Jong 1975
Goldberg 1989

- Evolution Strategies
Simple
CMA-ES
Natural Evolution Strategies
Williams REINFORCE 1992, expanded on in PEPG 2009 and Natrual Evolution Strategies 2014 - all saved on drive
OpenAI-ES, special case of REINFORCE-ES

- Textbooks
Haykin, NN: A Comprhensive Foundation (1994)
Introduction to Machine Learining

- Applications
Music Composition (Chen 2001)
Go (Richards, eta al 1998)
Robot Arm (D'Silva 2009)
Multiple Agents in Mobile ad-hoc Networks (Knoester 2010) (David B. Knoester, Heather Goldsby, Philip K. McKinley)
Multi-Agent Reinforcement Learning
Perception
NLP / LLM


- Reading List
Buckland 2002 - AI Techniques for Game Programming
Kohl 2009 - Learning in fractured problems... 
Computational Creativity (Boden 2006)
Open-Ended Evolution (Standish 2003, Bedau 2008)
Growing Adaptive Machines by D'Ambrosio (2014)

- Approaches
NEAT, Stanley and Mikkkulainen 2002
HyperNEAT, Stanley, et al 2009
RBF-NEAT
CascadeNEAT
SNAP-NEAT = Cascade + RBF
Novelty Search (Lehman and Stanley 2008,2011a)
NEAT-based methods are categorized based on 
Genetic Programming
Differential Evolution
Grammatical Evolution
Evolutionary Programming

- Implementations
SharpNEAT (C#) Green, 2004

- Companies / Groups
InstaDeep
BioNTech
Nvidia
Google DeepMind
Google Brain
OpenAI
Imperial College of Longon Adaptive and Intelligent Robotics Lab
Ecole Polytechnique Ferale de Lausanne Laboratory of Intelligent Systems- Switzerland
Naturally Inspired Computation Research Group
Institute for Experiential Robotics, Northeastern University
University Penn GRASP Laboratory
Entos, Inc (now Iambic)
Neural Propulsion systems, inc
Quebec Artificial Intelligence Institute
Learning in Machines & Brains CIFAR
Robot Perception and Learning Lab at UT Austin and Human Centerered Robotics Lab

- Journals and Blogs
Evolutionary Intelligence
Evolutionary Computation
Journal of Machine Learning Research
NeurIPS
Nature Machine Intelligence
Proceedings of ICLR

- People
Reisinger and Miikkulainen
Yamauchi and Beer
Blynel and Floreano
Edgar Galvan and Peter Mooney
Floreano et al and Yao
Stanley
Darwish
Anima Anandkumar
Yoshua Bengio and Geoffey Hinton
Chaowei Xiao (University of Wisconsin, Madison)
Yuke Zhu (Department of CS at University of Texas Austin)

Yoshua Bengio
	-  

- Gene Representations
Indirect, Direct, Implicit Encodings

- Dynamic Neuron Models
Continous-Time Recurrent Neural Networks - Universal Dynamic Approximators

- Benchmarks and Environments 
MNIST
Double Pole Balancing
Gym
Roboschool
PyBullet

- Interesecting Areas
Image Processing
Deep Reinforcement Learning
Mult-Agent Learning
Meta-Learning

- Problems in NE
Competing Conventions Problem - (Montana and Davis 1989, Shaffer 1992)
	- Having more than one way to express a solution, different encodings, leading to damaged offspring

- Timeline
1795 Gauss, 1805 Legendre, use method of least squares or linear regression for prediction of planetary movement
1925 Ising Model created
1949 Warren Mcculloch and Walter Pitts wrote a paper on how neurons might work
1949 Donald Hebb wrote "The Organization of Behavior", introduced Hebbian Learning
1952 Arthur Samuel coined the term Machine Learning. His work included alpha-beta pruing and the minimax algorithm
1959 Frank Rosenblatt combined Hebb's brain cell model with Arthur Samuel's ML and created the perceptron
1960 Henry J. Kelley and Arthur E. Bryson used princples of dynamic programming to derive method listed below
1962 Term "back-propogating error correction" introduced by Frank Rosenblatt. Precursor implemented by Henry J. Kelley in context of control theory 
1962 Stuart Dreyfus published simpler derivation of backprop based only on chain rule applied to networks of differentiable nodes
1965 Nilsson's "Learning Machines: Foundations of Trainable Pattern-Classifying Systems" published
1965 First deep learning MLP published by Ivakhnenko and Lapa
1967 Nearest Neighbor Algorithm, beginning basic pattern recognition
1969 Kunihiko Fukushima introduced ReLU activation function
1967 First deep learning multilayer perceptron trained by stochastic gradient descent published by Shun'ichi Amari
1972 Kohonen and Anderson independently developed a network utilizing matrix mathematics
1970s ML mostly interest in pattern recognition (Duda and Hart 1973)
1975 First multilayered NN developed
1980 CNNs introduced by Kunihiko Fukushima
1982 Paul Werbos applied backprop to MLPs in a way that became standard
1982 SOMs published by Teuvo Kohonen
1986 Rumelhart, Hinton, Williams showed backprop learned internal representations of words as feature vectores when trained
1987 TDNN introduced by Alex Waibel combining convolutions and weight sharing and backprop
1988 Wei Zhang applied backprop to a CNN
1988 NNs applied to protein structure prediction
1990 Boosting concept introduced by Robert Schapire in "The Strength of Weak Learnability"
1991 Juergen Schmidhuber published adversarial neural networks
1991 Sepp Hochreuter published his thesis identifying and analyzing the vanishing gradient problem and proposed recurrent residual connections solution
1992 Max-Pooling for CNNs was introduced by Juan Weng to help with least-shift invariance
1992 Juergen Schmidhuber proposed a hierarchy of RNNs pretrained one level at a time with self-supervised learning to overcome backpropogation's limititations on FNNs and RNNs. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network
1992 Juergen Schmidhuber introduced alternative to RNNs, called a linear transformer
1997 Hochreiter's work leads to Long Short-Term Memory deep-learning method published in Neural Computation
1998 7-level CNN by Yann LeCun applied to handwritten checks 
1999 "Vanilla" LSTM introduced by Felix Gers, Schmidhuber, Fred Cummins
2002 Evolving Neural Networks Through Augmenting Topologies published by Stanley and Miikkulainen
2005 Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used the LSTM principle to create the Highway network
2005 Kaiming He, Xiangyu Zhang; Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network
2007 Stanley publishes CPPN paper
2009 Stanley publishes "A hypercube-based encoding for evolving large-scale neural networks"
2009 Graves LSTM won three competitions in connected handwriting
2010 Curesan et al show that GPUs make backpropagation feasible for many-layered feedforward neural networks
2011 Abandoning objectives: Evolution through the search for novelty alone
GA's used to evolve weights of a fixed topology (1989 - 1999)
Topology Evolution (1994 - 2002)
2017 Stanley "Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning"
2017 Ashish Vaswani introduced modern transformers with their paper "Attention is All You Need"
2019 Stanley "Designing neural networks through neuroevolution"
2023 Backpropogation implemented on a photonic processor by a team at Stanford

- Research Paper Common Structures
<Some Improvement / Interesting Result > using <New Technique>
<Using X Technique> for <Some Improvement / Interesting Result>
<Applying X Technique> to <Different Kind of Problem Than Normally Studied>

- Notes from Papers
{	From "Comparison of NEAT and HyperNEAT on a Strategic Decison-Making Problem"
	HyperNEAT improves upon NEAT for relatively simple fractured problems, but the benefits disappear as the problem gets more complicated. This is based on this experiment only and should be tested on other problems
	Look into "fractured problems"
	Some work has been done to extend NEAT for performance on fractured problems. Kohl and Mikk (2009) demonstrated that NEAT's performance on several fractured problems can be improved if a mutation is added to the algorithm that allows the addition of radial basisc function nodes (RBF-NEAT). Also suggests that NEATs performance could be improved if its search space were constrained to cascade architectures, a network arrchitecture in which each hidden node is connected to all hidden nodes to its left in addition to inputs and outputs (Cascade-NEAT)
	More future work would be improving implementations for computer performance
	Machine Learning Technique - Random Decision Forests for determining for themselves when they've had enough training through the use of statistical hypothesis tests
}
{	From "Quality Diversity: A New Frontier for Evolutionary Computation"
	Natural evolution isn't just perfromance optimization, it is a divergent search that optimizes locally within each niche as it simultaenously diversifies. Discovery is of both quality and diversity -> Quality Diversity (QD) Algorithms
	Novelty Search (Stanley and Lehman 2008, 2011a) has effectively shown that evolutions talent for diversification can itself be harnessed as a powerful tool for seeking a near-optimum.
	Quaity Diversity seeks to find a maximally diverse collection of individuals in which each member is as high performing as possible. One algorithm is Novelty Search with Local Competition (Lehman and Stanley 2011b) and MAP-Elites (Mouret and Clune 2015)
	In QD, diversity between individuals is measured with respect to behavior, from which an experimenter selects some subset of behavioral features of interest to form a behavior characterization. These behavior spaces must be divided into niches that together cover the entire space.
	Mouret and Clune 2015 - QD effectively reveals the best possible performance achievable in each region of the phenotype space
}
{ From "A Systematic Literature Review of the Successors of ‚ÄúNeuroEvolution of Augmenting Topologies"
	NEAT-based methods are categorized based on 
		1. Whether they consider issues specific to the search space or fitness landscape
		2. Whether they combine principles from NE and another domain or
		3. The particular properties of the evolved ANNs
}
{	From "A Systematic Comparison of Simulation Software for ..."
	A recent example of a digital twin solution for a robotic
	arm can be found in [8] where the authors used ROS to achieve seamless operation between
	the real and digital world
	From our review and experimental results, we found
	that current robot simulation software could not be used
	to develop a digital twin. This is because the simulators
	considered in this paper cannot maintain a repeatable simulated scene over time. We hypothesise that a continuous
	feedback mechanism is needed between the simulation and
	reality similar to [22] in order to maintain an accurate
	representation of the real environment -> 22: ‚ÄúSim2Real2Sim: Bridging the Gap Between Simulation and Real-World in Flexible Object Manipulation
}
{	From "Neuroevolution Through Augmenting Topologies"
	- Many systems have been developed over the last decade that evolve both neural network topologies and weights (Angeline et al., 1993; Braun and Weisbrod, 1993; Dasgupta and McGregor, 1992; Fullmer and Miikkulainen, 1992; Gruau et al., 1996; Krishnan and Ciesielski, 1994; Lee and Kim, 1996; Mandischer, 1993; Maniezzo, 1994; Opitz
	and Shavlik, 1997; Pujol and Poli, 1998; Yao and Liu, 1996; Zhang and Muhlenbein, ¬®1993)
	- TWEANNs can be divided between those that use a direct encoding, and those that
	use an indirect one. Direct encoding schemes, employed by most TWEANNs, specify
	in the genome every connection and node that will appear in the phenotype (Angeline et al., 1993; Braun and Weisbrod, 1993; Dasgupta and McGregor, 1992; Fullmer and Miikkulainen, 1992; Krishnan and Ciesielski, 1994; Lee and Kim, 1996; Maniezzo,
	1994; Opitz and Shavlik, 1997; Pujol and Poli, 1998; Yao and Liu, 1996; Zhang and
	Muhlenbein, 1993). In contrast, indirect encodings usually only specify rules for con- ¬®
	structing a phenotype (Gruau, 1993; Mandischer, 1993).
	- Subgraph swapping is representative of a prevailing philosophy in TWEANNs that subgraphs are functional units and therefore swapping them
	makes sense because it preserves the structure of functional components
	- Several different problems with TWEANNs
}
{	From "Efficient Reinforcement Learning through Evolving Neural Network Topologies"
	- As of 2002, NE methods were the strongest method on the pole-balancing benchmark RL tasks, but evolving topology was not yet done much. 
	- NEAT is introduced focused on principled crossover, protection of structural innovation, and using incremental growth from minimal structure
	- NEAT, and NE generally is a way of learning with sparse reinforcement
	- Pole-balancing NE results were gained from evolving weights on fixed topologies
	- An open question then (answered now?) is whether Topology evolution can enhance performance, and if so if its worth the tradeoff of increased difficulty
	- Topology evolving method called Cellular encoding (Gruau 1996) was compared to ESP (Gomez and Miikulainen 1999). ESP was 5 times faster even tho it uses a random number of hidden nodes
	- NEAT's genetic encoding is nothing special other than its innovation number. It uses connection and node genes, with mutation occuring on the weights and addition of nodes. 
	
}

NEAT Framework
(1) Is there a genetic representation that allows
disparate topologies to crossover in a meaningful way? 
(2) How can topological innovation that needs a few generations to optimize be protected so that it does not disappear
from the population prematurely? 
(3) How can topologies be minimized throughout evolution without the need for a
specially contrived fitness function that measures complexity?

What if? :
+ Instead of tracking genes through historical markings and the innovation number, any gene can be tagged as a line up point, and what genes are or are not tagged is a part of speciation 
- When species reproduce, they will produce several candidate zygotes. Most will have inherited number of line up points, but some will have one extra, one less, or a shifted one. 
- If the reproducing agents have contributed any zygotes with the same # of lineup points, they will each contribute a zygote for crossover. Therefore speciation is defined by number of lineup points
+ There were some weight training / learning method applied on agents during their lifespan
- Some percentage of the difference between their natural weight and the end learned weight were applied to offspring
- When weights on a connection are decreased enough, the connection is killed. 
- When weights on a connection are increased enough, the connection splits into two neurons

- Elements of a Framework
Fixed morphology. Fixed features. Fixed set of inputs and outputs. How to create an environment with a rich or implict set of inputs and outputs?
	- Implicit caloric need based on brain complexity, how often they compute
	- Lifespan is based on 
	- Invest time or resources into elements that alter the environment
		- Lay down pheromone trails that can only be read by their own species
		- "Nests" that decrease reproduction after an initial investment
		- Selectively target species to cultivate / farm other species
	- Certain species will gravitate towards doing different things
Implicit energy usage / complexity score
Genomic representation of NN, can I evolve an encoding scheme?
Representation of input and output data to the cortex
Online learning, including learning implementations/strategies that can also evolve
Speciation. What about two genomes is sufficient or necessary for the ability to crossover
Sexual crossover of two genomes - how to

Ideally, I want every component of this to evolve.

ML Intuitions, Assumptions, and Main Questions

1. There is a spectrum to be explored between Supervised <----> Unsupervised Learning. They are not fundamentally different
2. Neural Networks, other ML models are best thought of as nonlinear system approximators. Best results is not finding some static optima on a topology, but a trajectory in a search space
3. Evolutionary programming is just a biology-inspired type of optimization. It could be generalized into the Supervised <--> Unsupervised spectrum and other optimization methods over a search space
4. Along with the point above, there is nothing magical about crossover or reproduction. Hyperparameter and parameter mixtures and inhereitance in a population of models can be made into an open, general approach
5. The best model would always come from starting with a simple model, simplified problem sets, and find a way to add complexity upward than to start with a complex problem and tune hyper parameters. 
6. Along with the point above, it is important to think about how data sets can be synthesized, augmented, reduced, etc to apply simpler models to and scale up, or to learn sub-features of a more complex behavior
7. Learning methods should be meta. A model should have a search space of learning and be "learning-how-to-learn" by sampling the search space. A human will never find the perfect learning method for every model and its a waste of time trying, though understanding the effects these hyperparameters have and how methods work is important. 
	7a. Learning methods are an integral part of the model, not an outside action on the model
8. Similar to point 1, the distinction between online <----> offline learning is a spectrum. Fully offline learning, a static model in production, will quickly be rendered obsolete in interesting, dynamic real world environments. Best results in real world enviornments, or any sort of AGI will be some sort of "online learning" on the spectrum
9. Best results, or a most general model, will be found with varying learning rates in a model. There is some subnet to complex behavior that would best be preserved and iterated, a baseline model to be found and applied across tasks in a similar space.
10. The complexity and size of the model is related to the complexity of the application in an interesting way worth exploring 
11. To expand on the above point will need more work. Basically, I think all image recognition models could be built off a very similar sub-model that is well versed and optimized at some set of universal/common subtasks like distance recognition, perspective, stuff like that. Scaling up to more sophisticated applications (and more sophisitcated models) could start from these submodels.
12. I should never focus on any specific application. Best learning results will be found sampling through all of them and generalizing
13. Data, the world of inputs within which your model lives, is a very deep and more general concept
14. Can we set/analyze an NN as nonlinear system with a set of trajectories and look at it that way? Can we analyze data that way, with sets of trajectories through it that give us consistent answers to it?