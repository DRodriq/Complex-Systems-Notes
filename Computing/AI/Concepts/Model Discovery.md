## Introduction
From the abstract of [[Automated Statistical Model Discovery with Language Models (Li, Fox, Goodman) (2024)]]: Statistical model discovery is a search over a vast space of models subject to domain-specific constraints. Efficiently searching this space requires expertise in modeling and the problem domain.
## Relation to Complexity
Lets consider the basic logistics equation with the parameter r. 
Recall the discrete logistics map equation: $x_{t}=rx_{t-1}(1-x_{t-1})$ 

![[bifurcation_diagram_logistics_equation.png]]Here we see the period-doubling route to chaos as r is increased. 
Now consider model discovery for some dynamical system using neural networks. It seems clear to me that a neural network could learn the parameters of the logistics equation here in the simple cases and in chaos - the neural network can simulate the basic dynamics, the chaos, and everything in between. The everything in between (edge-of-chaos) is what is important, I think.
I am trying to recall where I read it, but one idea behind the edge-of-chaos behavior of many complex adaptive systems is that we can imagine the trade-off between an extremely stable behavior within the possible solution space, and a more or less stable sampling over that solution space. A complex adaptive system or other evolutionary system wants to be stable around some dynamics that "work," but also explore new areas of the solution space in case they are more optimal/fit. How to balance that? Chaos is.. chaos, you are unlikely to just land on a new area of the solution space that is more fit. Not evolving is obviously not ideal either, so you balance the two by being "on the edge of chaos," sampling the solution space in some balance.
So imagine a dynamical system at the edge of chaos. Minor differences to the parameters allows an exploration of the solution space without straying too far. Positive outcomes will drift the system towards that exploration. Failures will die off. A neural network performing model discovery in such a way will better evolve.
There are two sides to the coin here between model knowledge: being able to frame out the broad strokes of the dynamics of the system, and the neural network filling in the gaps and better approaching parameters, reducing the dimensions of the problem, or adding a new dimension that allows a more efficient solution of the problem. Human modeler and aritifical system can then work together towards a more efficient modeling of the problem / solution space. The modeler could perhaps give the neural network more of a leg-up so it is not starting from some random soup of math and random weights / dimensions and instead be quite close to the dynamics being studied, thus massively increasing learning efficiency from the standpoint of how much data is needed to train, and the end performance.
